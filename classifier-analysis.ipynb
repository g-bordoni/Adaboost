{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Any, Tuple\n",
    "from classifier.my_adaboost import MyAdaboostClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import csv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tic_tac_toe_data() -> Tuple[list, list]:\n",
    "    \"\"\"\n",
    "        Funcao para ler o arquivo de entrada e retornar os dados em forma\n",
    "        de listas numericas\n",
    "    \"\"\"\n",
    "    \n",
    "    def category_convertor(item):\n",
    "        if item == 'x' or item == 'positive':\n",
    "            return 1\n",
    "        if item == 'o' or item == 'negative':\n",
    "            return -1\n",
    "        if item == 'b':\n",
    "            return 0\n",
    "        raise ValueError\n",
    "\n",
    "    feature_values, label_values = list(), list()\n",
    "    with open('tic-tac-toe.data', newline='') as csv_file:\n",
    "        reader = csv.reader(csv_file)\n",
    "        for row in reader:\n",
    "            feature_values.append(np.array([category_convertor(item) for item in row[:-1]]))\n",
    "            label_values.append(category_convertor(row[-1]))\n",
    "            \n",
    "    return feature_values, label_values\n",
    "\n",
    "\n",
    "def create_k_partitions(features: list, labels: list, k=5, random_seed=42) -> Tuple[List[list], List[list]]:\n",
    "    \"\"\"\n",
    "        Funcao para criar k particoes dos dados de entrada para realizar\n",
    "        a validacao cruzada\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(random_seed)\n",
    "    partition_size = len(features) // k\n",
    "    \n",
    "    features_partitions = list()\n",
    "    labels_partitions = list()\n",
    "    for _ in range(k):\n",
    "        f_partition = list()\n",
    "        l_partition = list()\n",
    "        while len(f_partition) < partition_size:\n",
    "            item_idx = np.random.randint(len(features))\n",
    "            f_partition.append(features.pop(item_idx))\n",
    "            l_partition.append(labels.pop(item_idx))\n",
    "        features_partitions.append(f_partition)\n",
    "        labels_partitions.append(l_partition)\n",
    "\n",
    "    return features_partitions, labels_partitions\n",
    "\n",
    "\n",
    "def cross_validation_score(model: Any, features_partitions: List[list], labels_partitions: List[list]) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "        Funcao para calcular a taxa de acertos do classificador com seu \n",
    "        respectivo desvio padrao por meio da validacao cruzada\n",
    "    \"\"\"\n",
    "    \n",
    "    partitions_size = len(features_partitions)\n",
    "    partitions_accuracy_score = []\n",
    "    for i in range(partitions_size):\n",
    "        features_to_test = features_partitions[i]\n",
    "        labels_to_test = labels_partitions[i]\n",
    "        features_to_train = list()\n",
    "        labels_to_train = list()\n",
    "        for j in range(partitions_size):\n",
    "            if i != j:\n",
    "                labels_to_train.extend(labels_partitions[j])\n",
    "                features_to_train.extend(features_partitions[j])\n",
    "        model.fit(features_to_test, np.array(labels_to_test))\n",
    "        partitions_accuracy_score.append(model.score(features_to_train, np.array(labels_to_train)))\n",
    "    partitions_accuracy_score = np.array(partitions_accuracy_score)\n",
    "    \n",
    "    return partitions_accuracy_score.mean(), np.sqrt(partitions_accuracy_score.var())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8628272251308902, 0.022631491881205244)\n",
      "(0.8900523560209426, 0.0200565249312858)\n"
     ]
    }
   ],
   "source": [
    "features, labels = get_tic_tac_toe_data()\n",
    "features_partitions, labels_partitions = create_k_partitions(features, labels, k=5)\n",
    "\n",
    "# model_x = MyAdaboostClassifier(n_estimators=100, classifier_model='weak-logistic-regression')\n",
    "model_x = MyAdaboostClassifier(n_estimators=100, classifier_model='stump')\n",
    "model_y = AdaBoostClassifier(n_estimators=100)\n",
    "\n",
    "dd = cross_validation_score(model_x, features_partitions, labels_partitions)\n",
    "print(dd)\n",
    "dd = cross_validation_score(model_y, features_partitions, labels_partitions)\n",
    "print(dd)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7a0138cbc5ab2fb617aa07fda9a1f3f75d0ae9d49d28f34e144dc6f98a2db0b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
